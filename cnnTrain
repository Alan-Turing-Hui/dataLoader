# 设备配置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 定义损失函数
criterion = nn.BCELoss()  # 二元交叉熵损失

import time
import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.001)

#定义训练函数
def train(model, train_loader, epoch, optimizer, metrics_history):
    model.train()  # 设置模型为训练模式
    running_loss = 0.0
    correct = 0  # 用于计算训练准确率
    total = 0    # 总样本数

    # 开始记录训练时间
    start_time = time.time()

    for batch_idx, (data, labels) in enumerate(train_loader):
        # 将数据和目标转移到设备
        data, labels = data.to(device), labels.to(device).float()  # 确保目标是浮点数

        # 初始化隐藏状态
        hidden = model.init_hidden(data.size(0))

        # 清零梯度
        optimizer.zero_grad()

        # 前向传播
        outputs, hidden = model(data, hidden)

        # 计算损失
        loss = criterion(outputs, labels.unsqueeze(1))  # 添加一个维度以匹配输出形状

        # 反向传播和优化
        loss.backward()
        optimizer.step()

        # 累加损失
        running_loss += loss.item()

        # 计算准确率
        predicted = (outputs >= 0.5).float()  # 阈值 0.5，将输出转为0或1
        total += labels.size(0)
        correct += (predicted.squeeze() == labels).sum().item()

        # 输出每个 epoch 的损失
        if batch_idx % 10 == 0:  # 每 10 个 batch 打印一次
            print(f'epoch [{epoch+1}/{num_epochs}], step [{batch_idx}/{len(train_loader)}], loss: {loss.item():.4f}')

    # 计算训练时间
    train_time = time.time() - start_time

    # 计算平均损失
    average_loss_train = running_loss / len(train_loader)
    
    # 计算准确率
    accuracy_train = 100 * correct / total

    # 将损失和准确率保存到 metrics_history 中
    metrics_history['train_loss'].append(average_loss_train)
    metrics_history['train_accuracy'].append(accuracy_train)

    print(f'epoch [{epoch+1}/{num_epochs}] - average_loss_train: {average_loss_train:.4f}, accuracy_train: {accuracy_train:.2f}%')

    return train_time # 返回训练时间
